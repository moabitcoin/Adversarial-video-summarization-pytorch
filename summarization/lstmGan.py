import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm

from summarization.Discriminator import Discriminator
from summarization.models import Summarizer
from tools.dataset import DatasetParser
from tools.tf_log_writer import LogWriter
from tools.utils import my_collate


class LstmGan(object):
  def __init__(self, config=None):
    self.config = config
    self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

  # build lstm-gan model
  def build(self):
    self.summarizer = Summarizer(self.config.input_size,
                                 self.config.hidden_size,
                                 self.config.num_layers)
    self.summarizer = self.summarizer.to(self.device)
    self.summarizer = nn.DataParallel(self.summarizer)

    # build Discriminator
    self.discriminator = Discriminator(self.config.input_size,
                                       self.config.hidden_size,
                                       self.config.num_layers)
    self.discriminator = self.discriminator.to(self.device)
    self.discriminator = nn.DataParallel(self.discriminator)
    # hold and register submodules in a list
    self.model = nn.ModuleList([self.summarizer, self.discriminator])

    if self.config.mode == 'train':
      # build optimizers
      self.s_e_optimizer = optim.Adam(
        list(self.summarizer.module.slstm.parameters()) +
        list(self.summarizer.module.vae.elstm.parameters()),
        lr=self.config.sum_learning_rate)
      self.d_optimizer = optim.Adam(
        list(self.summarizer.module.vae.dlstm.parameters()),
        lr=self.config.sum_learning_rate)
      self.c_optimizer = optim.Adam(
        list(self.discriminator.parameters()),
        lr=self.config.dis_learning_rate)

      # set the model in training mode
      self.model.train()

      # initialize log writer
      self.writer = LogWriter(self.config.log_dir)

  """
  reconstruct loss
  Args:
      fea_h_last: given original vodieo feature, the output of the last 
      hidden (top) layer of cLSTM, (1, hidden) = (1, 1024)
      dec_h_last: given decoded video feature, the output of the last hidden 
      (top) layer of cLSTM, (1, hidden) = (1, 1024)
  """

  def get_reconst_loss(self, fea_h_last, dec_h_last):
    # L-2 norm
    return torch.norm(fea_h_last - dec_h_last, p=2)

  """
  summary-length regularization
  Args:
      scores: (seq_len, 1) 
  """

  def get_sparsity_loss(self, scores):
    # convert scores to 0-1
    # scores = scores.ge(0.5).float()
    return torch.abs(torch.mean(scores) - self.config.summary_rate)

  """
  dpp loss
  """

  def get_dpp_loss(self, scores):
    seq_len = len(scores)
    dpp_loss = 0
    for i in range((seq_len - 1)):
      dpp_loss += (scores[i] * scores[i + 1] + (1 - scores[i]) * (
              1 - scores[i + 1]))

    return torch.log(1 + dpp_loss)

  """
  gan loss of cLSTM
  Args:
      fea_prob: a scalar of original video feature
      dec_prob: a scalar of keyframe-based reconstruction video feature
      rand_dec_prob: a scalar of random-frame-based reconstruction video feature
  """

  def get_gan_loss(self, fea_prob, dec_prob, rand_dec_prob):
    gan_loss = torch.log(fea_prob) + torch.log(1 - dec_prob) + torch.log(
      1 - rand_dec_prob)

    return gan_loss

  # train model
  def train(self):
    step = 0
    train_parser = DatasetParser(self.config.features_list)
    # custom collate to ignore none features
    dl = DataLoader(train_parser, batch_size=self.config.batch_size,
                    shuffle=True,
                    num_workers=self.config.num_workers, collate_fn=my_collate)

    for epoch_i in range(self.config.n_epochs):
      s_e_loss_history = []
      d_loss_history = []
      c_loss_history = []
      # one video is a batch
      pbar = tqdm(dl)
      for batch_i, b in enumerate(pbar):
        # feature: (1, seq_len, input_size) -> (seq_len, 1, 2048)
        (_, inputs, inputs_rev) = b

        inputs = inputs.view(-1, 1, self.config.input_size)
        inputs = inputs.to(self.device)

        # inputs_rev = inputs_rev.transpose(0, 1)

        # inputs_rev = inputs_rev.to(self.device)

        """ train sLSTM and eLSTM """
        if self.config.detail_flag:
          tqdm.write('------Training sLSTM and eLSTM------')

        # decoded: decoded feature generated by dLSTM
        scores, decoded = self.summarizer.module(inputs)
        # shape of feature and decoded: (seq_len, 1, 2048)
        # shape of fea_h_last and dec_h_last: (1, hidden_size) = (1, 2048)
        fea_h_last, fea_prob = self.discriminator.module(inputs)
        dec_h_last, dec_prob = self.discriminator.module(decoded)

        tqdm.write('feature_prob: %.3f, decoded_prob: %.3f}' % (
          fea_prob.item(), dec_prob.item()))

        # reconstruction loss
        reconst_loss = self.get_reconst_loss(fea_h_last, dec_h_last)
        tqdm.write('reconst_loss: %.3f' % reconst_loss.item())

        # sparsity loss
        sparsity_loss = self.get_sparsity_loss(scores)
        tqdm.write('sparsity_loss: %.3f' % sparsity_loss.item())

        # dpp loss
        dpp_loss = self.get_dpp_loss(scores)
        tqdm.write('diversity_loss: %.3f' % dpp_loss.item())

        # minimize
        s_e_loss = reconst_loss + sparsity_loss + dpp_loss

        self.s_e_optimizer.zero_grad()
        s_e_loss.backward(retain_graph=True)
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.clip)
        self.s_e_optimizer.step()

        # add to loss history
        s_e_loss_history.append(s_e_loss.data.cpu())

        """ train dLSTM """
        if self.config.detail_flag:
          tqdm.write('------Training dLSTM------')

        # randomly select a subset of frames
        _, rand_decoded = self.summarizer.module(inputs, random_score_flag=True)
        # shape of rand_dec_h_last: (1, hidden_size) = (1, 2048)
        rand_dec_h_last, rand_dec_prob = self.discriminator.module(rand_decoded)
        # gan loss
        gan_loss = self.get_gan_loss(fea_prob, dec_prob, rand_dec_prob)
        tqdm.write('gan_loss: %.3f' % gan_loss.item())

        # minimize => # 2. For learning θd, minimize (Lreconst+LGAN).
        d_loss = reconst_loss + gan_loss

        self.d_optimizer.zero_grad()
        d_loss.backward(retain_graph=True)
        # gradient clipping
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.clip)
        self.d_optimizer.step()

        # add to loss history
        d_loss_history.append(d_loss.data.cpu())

        """ train cLSTM """
        if batch_i > self.config.dis_start_batch:
          if self.config.detail_flag:
            tqdm.write('------Training cLSTM------')

        # maximize
        # we use negative sign for the loss  because they need to
        # be maximized, whereas pytorch’s optimizer can only do minimization.
        c_loss = -1 * self.get_gan_loss(fea_prob, dec_prob, rand_dec_prob)

        self.c_optimizer.zero_grad()
        # gradient clipping
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.clip)
        c_loss.backward()
        self.c_optimizer.step()

        # add to loss history
        c_loss_history.append(c_loss.data.cpu())

        if self.config.detail_flag:
          tqdm.write('------update tensorboard------')

        self.writer.update_loss(reconst_loss.cpu().data.numpy(), step,
                                'reconst_loss')
        self.writer.update_loss(sparsity_loss.cpu().data.numpy(), step,
                                'sparsity_loss')
        self.writer.update_loss(gan_loss.cpu().data.numpy(), step, 'gan_loss')

        self.writer.update_loss(fea_prob.cpu().data.numpy(), step, 'fea_prob')
        self.writer.update_loss(dec_prob.cpu().data.numpy(), step, 'dec_prob')
        self.writer.update_loss(rand_dec_prob.cpu().data.numpy(), step,
                                'rand_dec_prob')

        # bacth end
        step += 1

      # epoch
      s_e_loss = torch.stack(s_e_loss_history).mean()
      d_loss = torch.stack(d_loss_history).mean()
      c_loss = torch.stack(c_loss_history).mean()

      if self.config.detail_flag:
        tqdm.write('------update tensorboard------')
      self.writer.update_loss(s_e_loss, epoch_i, 's_e_epoch')
      self.writer.update_loss(d_loss, epoch_i, 'd_loss_epoch')
      self.writer.update_loss(c_loss, epoch_i, 'c_loss_epoch')

      # save parameters at checkpoint
      model_path = str(self.config.model_save_dir) + '/' + 'sum-model.pkl'
      tqdm.write('------save model parameters at %s' % model_path)
      torch.save(self.model.state_dict(), model_path)
